# Copy this file to .env and fill in your values
# Ollama Configuration (for local LLM inference)
# If using cloud deployment, you may need to use cloud-based LLM services instead
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3:8b
OLLAMA_MODEL_COMPARISON=meditron
ENABLE_MODEL_COMPARISON=false

# Perplexity API (for publication search)
# Get your API key from https://www.perplexity.ai/ (Account Settings â†’ API)
PERPLEXITY_API_KEY=your_perplexity_api_key_here

# Unpaywall API (for open access PDF retrieval)
UNPAYWALL_EMAIL=your_email@example.com

# API Configuration
# For production, set this to your deployed backend URL
API_URL=http://localhost:8000

# CORS Configuration (comma-separated list of allowed origins)
# For production, add your frontend URL(s) here
ALLOWED_ORIGINS=http://localhost:3000,https://your-frontend-domain.vercel.app

# Model Comparison Limits
MAX_COMPARE_MODELS=5
MAX_CONCURRENT_MODELS=3

# Redis Configuration (optional, for caching)
# If using Redis for caching in production
REDIS_URL=redis://localhost:6379
REDIS_PASSWORD=

# Security
# Generate a secure secret key for production
SECRET_KEY=your-secret-key-here-change-in-production
ENVIRONMENT=development

# Logging
LOG_LEVEL=INFO

# Database (if you add one later)
# DATABASE_URL=postgresql://user:password@localhost:5432/dbname

